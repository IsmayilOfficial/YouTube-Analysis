{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding sections for youtube.com NewOnNetflix\n",
      "  getting playlists for section: Trailers\n",
      "    getting videos for playlist: Animated Series | Netflix\n",
      "      processing video 'Invader Zim: Enter the Florpus | Official Trailer | Netflix'\n",
      "      processing video 'Enter The Anime | Official Trailer | Netflix'\n",
      "      processing video 'Burn Your Cosmo | Saint Seiya | NX'\n",
      "      processing video 'Saint Seiya: Knights of the Zodiac | Official Trailer | Netflix'\n",
      "      processing video 'Kengan Ashura | Official Trailer | Netflix'\n",
      "      processing video 'Aggretsuko: Season 2 Clip: White-hot Rap Battle'\n",
      "      processing video 'Aggretsuko: Season 2 | Official Trailer | Netflix'\n",
      "      processing video '7SEEDS | Official Trailer | Netflix'\n",
      "      processing video 'Rilakkuma and Kaoru | Interview with Creators | NX on Netflix'\n",
      "      processing video 'Ultraman | Interview with Creators | Netflix'\n",
      "      processing video 'Tuca & Bertie | Official Trailer [HD] | Netflix'\n",
      "      processing video 'LOVE DEATH + ROBOTS | Inside the Animation: Shape-shifters | Netflix'\n",
      "      processing video 'LOVE DEATH + ROBOTS | Inside the Animation: Helping Hand | Netflix'\n",
      "      processing video 'LOVE DEATH + ROBOTS | Directing Samira Wiley | Netflix'\n",
      "      processing video 'LOVE DEATH + ROBOTS | Inside the Animation: Sucker of Souls | Netflix'\n",
      "      processing video 'LOVE DEATH + ROBOTS | Inside the Animation: Good Hunting | Netflix'\n",
      "      processing video 'LOVE DEATH + ROBOTS | Creating the CG Sex Scene | Netflix'\n",
      "      processing video 'LOVE DEATH + ROBOTS | Inside the Animation: Blindspot | Netflix'\n",
      "      processing video 'LOVE DEATH + ROBOTS | Inside the Animation: Suits | Netflix'\n",
      "      processing video 'LOVE DEATH + ROBOTS | Live Horse Motion Capture | Netflix'\n",
      "      processing video 'LOVE DEATH + ROBOTS | Inside the Animation: The Secret War | Netflix'\n",
      "      processing video 'LOVE DEATH + ROBOTS | Inside the Animation: Sonnie‚Äôs Edge | Netflix'\n",
      "      processing video 'LOVE DEATH + ROBOTS | Inside the Animation: Fish Night | Netflix'\n",
      "      processing video 'LOVE DEATH + ROBOTS | Inside the Animation: Three Robots | Netflix'\n",
      "      processing video 'YouTube'\n",
      "      processing video 'LOVE DEATH + ROBOTS | Inside the Animation: The Witness | Netflix'\n",
      "      processing video 'LOVE DEATH + ROBOTS | Inside the Animation: Zima Blue | Netflix'\n",
      "      processing video 'LOVE DEATH + ROBOTS | Inside the Animation: Tim Miller | Netflix'\n",
      "      processing video 'LOVE DEATH + ROBOTS | How to Tame a Cat | Netflix'\n",
      "      processing video 'Tuca & Bertie | Equal Pay Day | Netflix Is A Joke'\n",
      "      processing video 'Neon Genesis Evangelion | Date Announcement | NX on Netflix'\n",
      "      processing video '7Seeds | Teaser [HD] | Netflix'\n",
      "      processing video 'Levius | Teaser [HD] | Netflix'\n",
      "      processing video 'Rilakkuma and Kaoru | Official Trailer [HD] | Netflix'\n",
      "      processing video 'LOVE DEATH + ROBOTS ‚ù§Ô∏èüíÄü§ñ | Live Countdown | Mature Audiences Only | Netflix'\n",
      "      processing video 'Living Life the Coach Steve Way | Big Mouth | Netflix'\n",
      "      processing video 'LOVE DEATH + ROBOTS | üíÄTrailer [HD] | Netflix'\n",
      "      processing video 'Big Mouth | Jay Falls in Love with His Pillow | Netflix'\n",
      "      processing video 'Ultraman | Official Trailer [HD] | Netflix'\n",
      "      processing video 'LOVE DEATH + ROBOTS | ‚ù§Ô∏èTrailer [HD] | Netflix'\n",
      "      processing video 'Green Eggs and Ham: Season 1 | Teaser [HD] | Netflix'\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "\n",
    "# version 1.1 added handling of youtube.com/channel/\n",
    "#  to already handling of youtube.com/user based channels\n",
    "\n",
    "# NOTE: url gets to youtube are throttled to 3 seconds between requests\n",
    "# this is an ad hoc attempt to look like a human to youtube\n",
    "# so youtube does not start limiting access\n",
    "wait_between_requests = 3\n",
    "\n",
    "\"\"\" scrape youtube channel to build table of contents html file and \n",
    "    csv of video information for excel file\n",
    "    note this code has a slow down delay to meet youtube terms of use\n",
    "\"\"\"\n",
    "\n",
    "# set youtube channel name here\n",
    "channel_name = 'NewOnNetflix'\n",
    "\n",
    "\n",
    "youtube_base = 'https://www.youtube.com/'\n",
    "# others to try\n",
    "#  gotreehouse  howgrowvideo  gjenkinslbcc howgrowvideo\n",
    "# by channel name:\n",
    "#  UCu8YylsPiu9XfaQC74Hr_Gw  UCn34N9fj3x92kOGnQQdHkAQ\n",
    "# UCH4aPBlmmW1Vgs0ykktCMUg\n",
    "\n",
    "parent_folder = ''  # users or channel or empty\n",
    "\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"open url and return BeautifulSoup object, \n",
    "       or None if site does not exist\"\"\"\n",
    "    \n",
    "    result = requests.get(url)\n",
    "    if result.status_code != 200: return None\n",
    "    time.sleep(wait_between_requests)  # slow down to human speed\n",
    "    return BeautifulSoup(result.text, 'html.parser')\n",
    "\n",
    "\n",
    "def channel_section_links(channel_name):\n",
    "    \"\"\"list of \n",
    "       { 'title': <section title>, \n",
    "         'link': <url to section play lists> \n",
    "       }\"\"\"\n",
    "\n",
    "    global parent_folder\n",
    "\n",
    "    soup = get_soup(f'{youtube_base}/user/{channel_name}/playlists')\n",
    "    if soup is None or 'This channel does not exist.' in soup.text:\n",
    "        url = f'{youtube_base}/channel/{channel_name}/playlists'\n",
    "        soup = get_soup(url)\n",
    "        if soup is None or 'This channel does not exist.' in soup.text:\n",
    "            raise ValueError(\n",
    "                'The channel does not exists: ' + channel_name)\n",
    "        parent_folder = 'channel/'\n",
    "\n",
    "    play_list_atags = \\\n",
    "        soup.find_all('a',\n",
    "                      {'href': re.compile(f'{channel_name}/playlists')})\n",
    "    # filter out non user play lists next\n",
    "    elements = [{'title': x.text.strip(),\n",
    "                 'link': fix_url(x['href'])} for x in play_list_atags\n",
    "                if x.span and\n",
    "                ('shelf_id=0' not in x['href'])]\n",
    "\n",
    "    # no sections, make up no sections section with default link\n",
    "    if len(elements) == 0:\n",
    "        url = f'{youtube_base}{parent_folder}{channel_name}/playlists'\n",
    "        elements = [ {'title': 'no sections', 'link': url}]\n",
    "        # i.e.  https://youtube.com/gotreehouse/playlists\n",
    "    return elements\n",
    "\n",
    "\n",
    "def fix_url(url):  # correct relative urls back to absolute urls\n",
    "    if url[0] == '/':\n",
    "        return youtube_base + url\n",
    "    else:\n",
    "        return url\n",
    "\n",
    "\n",
    "def get_playlists(section):\n",
    "    \"\"\"returns list of list of\n",
    "    { 'title': <playlist tile>, <link to all playlist videos> }\"\"\"\n",
    "    global parent_folder\n",
    "    print(f\"  getting playlists for section: {section['title']}\")\n",
    "\n",
    "    soup = get_soup(section['link'])\n",
    "    if soup is None: # no playlist, create dummy with default link\n",
    "        url = f'{youtube_base}{parent_folder}{channel_name}/videos'\n",
    "        return [\n",
    "           {'title': 'No Playlists', 'link':url }]\n",
    "    atags = soup('a', class_='yt-uix-tile-link')\n",
    "\n",
    "    playlists = []\n",
    "    for a in atags:  # find title and link\n",
    "        title = a.text\n",
    "        if title != 'Liked videos': # skip these\n",
    "            url = fix_url(a['href'])\n",
    "            playlists.append({'title': title, 'link': url})\n",
    "\n",
    "    if not playlists:  # no playlists\n",
    "        url = f'{youtube_base}/{parent_folder}{channel_name}/videos'\n",
    "        return [{'title': 'No Playlists', 'link': url}]\n",
    "\n",
    "    return playlists\n",
    "\n",
    "def parse_video(vurl):\n",
    "    # return dict of\n",
    "    # title, link, views, publication_date,\n",
    "    # description, short_link, likes, dislikes\n",
    "\n",
    "    d = {'link': vurl, 'views': None, 'short_link': vurl,\n",
    "         'likes': None, 'dislikes': None}\n",
    "\n",
    "    # now get video page and pull information from it\n",
    "    vsoup = get_soup(vurl)\n",
    "\n",
    "    o = vsoup.find('title')\n",
    "    vtitle = o.text.strip()\n",
    "    xending = ' - YouTube'\n",
    "    d['title'] = vtitle[:-len(xending)] \\\n",
    "        if vtitle.endswith(xending) else vtitle\n",
    "    print(f\"      processing video '{d['title']}'\" )\n",
    "\n",
    "    # o is used in the code following to\n",
    "    # catch missing data targets for scrapping\n",
    "    o = vsoup.find('div', class_='watch-view-count')\n",
    "    if o:\n",
    "        views = o.text\n",
    "        d['views'] = ''.join(c for c in views if c in '0123456789')\n",
    "\n",
    "    o = vsoup.find('strong', class_='watch-time-text')\n",
    "    d['publication_date'] = \\\n",
    "        o.text[len('Published on ') - 1:] if o else ''\n",
    "\n",
    "    o = vsoup.find('div', id='watch-description-text')\n",
    "    d['description'] = o.text if o else ''\n",
    "\n",
    "    o = vsoup.find('meta', itemprop='videoId')\n",
    "    if o:\n",
    "        vid = o['content']\n",
    "        d['short_link'] = f'https://youtu.be/{vid}'\n",
    "\n",
    "    o = vsoup.find('button',\n",
    "                   class_='like-button-renderer-like-button')\n",
    "    if o:\n",
    "        o = o.find('span', class_='yt-uix-button-content')\n",
    "        d['likes'] = o.text if o else ''\n",
    "\n",
    "    o = vsoup.find('button',\n",
    "                   class_='like-button-renderer-dislike-button')\n",
    "    if o:\n",
    "        o = o.find('span', class_='yt-uix-button-content')\n",
    "        d['dislikes'] = o.text if o else ''\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def add_videos(playlist):\n",
    "    \"\"\"find videos in playlist[link]\n",
    "    and add their info as playlist[videos] as list\"\"\"\n",
    "    surl = playlist['link']\n",
    "    soup = get_soup(surl)\n",
    "    print(f\"    getting videos for playlist: {playlist['title']}\")\n",
    "\n",
    "    videos = []\n",
    "\n",
    "    # items are list of video a links from list\n",
    "    items = soup('a', class_='yt-uix-tile-link')\n",
    "\n",
    "    # note first part of look get info from playlist page item,\n",
    "    # and the the last part opens the video and gets more details\n",
    "    if len(items) > 0:\n",
    "        for i in items:\n",
    "            d = dict()\n",
    "            vurl = fix_url(i['href'])\n",
    "            t = i.find_next('span', {'aria-label': True})\n",
    "            d['time'] = t.text if t else 'NA'\n",
    "\n",
    "            d.update(parse_video(vurl))\n",
    "            videos.append(d)\n",
    "\n",
    "    else:  # must be only one video\n",
    "        d = {'time': 'NA'}\n",
    "        d.update(parse_video(surl))\n",
    "        videos.append(d)\n",
    "\n",
    "    # add new key to this playlist of list of video infos\n",
    "    playlist['videos'] = videos\n",
    "    print()\n",
    "\n",
    "\n",
    "def tag(t,c):\n",
    "    return f'<{t}>{c}</{t}>' # return html tag with content\n",
    "\n",
    "\n",
    "def link(text, url): # return a tag with content and link\n",
    "    return f'<a href=\"{url}\">{text}</a>'\n",
    "\n",
    "\n",
    "def html_out(channel, sections):\n",
    "    \"\"\"create and write channel_name.html file\"\"\"\n",
    "    title = f'YouTube Channel {channel}'\n",
    "     \n",
    "\n",
    "    f = open(f'{channel}.html','w',encoding=\"utf-8\")\n",
    "    template = ('<!doctype html>\\n<html lang=\"en\">\\n<head>\\n'\n",
    "                '<meta charset=\"utf-8\">'\n",
    "                '<title>{}</title>\\n</head>\\n'\n",
    "                '<body>\\n{}\\n</body>\\n</html>')\n",
    "\n",
    "    parts = list()\n",
    "    parts.append(tag('h1', title))\n",
    "\n",
    "    for s in sections:\n",
    "        parts.append(tag('h2',link(s['title'], s['link'])))\n",
    "        for pl in s['playlists']:\n",
    "            parts.append(tag('h3', link(pl['title'], pl['link'])))\n",
    "            if len(pl) == 0:\n",
    "                parts.append('<p>Empty Playlist</p>')\n",
    "            else:\n",
    "                parts.append('<ol>')\n",
    "                for v in pl['videos']:\n",
    "                    t = '' if v['time'] == 'NA' else f\" ({v['time']})\"\n",
    "                    parts.append(tag('li', link(v['title'],\n",
    "                                     v['short_link']) + t))\n",
    "                parts.append('</ol>')\n",
    "    f.write(template.format(channel, '\\n'.join(parts)))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def csv_out(channel, sections):\n",
    "    \"\"\" create and output channel_name.csv\n",
    "    file for import into a spreadsheet or DB\"\"\"\n",
    "    headers = ('channel,section,playlist,video,'\n",
    "               'link,time,views,publication date,'\n",
    "               'likes,dislikes,description').split(',')\n",
    "\n",
    "    with open(f'{channel}.csv', 'w', encoding=\"utf-8\") as csv_file:\n",
    "        csvf = csv.writer(csv_file, delimiter=',')\n",
    "        csvf.writerow(headers)\n",
    "        for section in sections:\n",
    "            for playlist in section['playlists']:\n",
    "                for video in playlist['videos']:\n",
    "                    v = video\n",
    "                    line = [channel,\n",
    "                            section['title'],\n",
    "                            playlist['title'],\n",
    "                            v['title']]\n",
    "                    line.extend([v['short_link'],\n",
    "                                 v['time'], v['views'],\n",
    "                                 v['publication_date'],\n",
    "                                 v['likes'], v['dislikes'],\n",
    "                                 v['description']])\n",
    "                    csvf.writerow(line)\n",
    "\n",
    "def process_channel(channel_name):\n",
    "    sections = channel_section_links(channel_name)\n",
    "    for section in sections:\n",
    "        section['playlists'] = get_playlists(section)\n",
    "        for playlist in section['playlists']:\n",
    "            add_videos(playlist)\n",
    "    return sections\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # find channel name by going to channel\n",
    "    # and picking last element from channel url\n",
    "    # for example my channel url is:\n",
    "    #   https://www.youtube.com/user/gjenkinslbcc\n",
    "    # my channel name is gjenkinslbcc in this url\n",
    "    # this is set near top of this file\n",
    "    # if the channel is of the form:\n",
    "    # https://www.youtube.com/channel/xyz then supply xyz\n",
    "\n",
    "    print(f'finding sections for youtube.com {channel_name}')\n",
    "    sections = process_channel(channel_name)\n",
    "\n",
    "    # save sections structure to json file\n",
    "    with open(f'{channel_name}.json','w') as f:\n",
    "        f.write(json.dumps(sections, sort_keys=True, indent=4))\n",
    "\n",
    "    html_out(channel_name, sections)  # create web page of channel links\n",
    "\n",
    "    # create a csv file of video info for import into spreadsheet\n",
    "    csv_out(channel_name, sections)\n",
    "\n",
    "    print(f\"Program Complete,\\n  '{channel_name}.html' and\"\n",
    "          f\" '{channel_name}.csv' have been\" \n",
    "          f\" written to current directory\")\n",
    "    # 3456789012345678901234567890123456789012345678901234567890123456789012345678901234567890\n",
    "    #        1         2         3         4         5         6         7         8         9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "\n",
    "# version 1.1 added handling of youtube.com/channel/\n",
    "#  to already handling of youtube.com/user based channels\n",
    "\n",
    "# NOTE: url gets to youtube are throttled to 3 seconds between requests\n",
    "# this is an ad hoc attempt to look like a human to youtube\n",
    "# so youtube does not start limiting access\n",
    "wait_between_requests = 3\n",
    "\n",
    "\"\"\" scrape youtube channel to build table of contents html file and \n",
    "    csv of video information for excel file\n",
    "    note this code has a slow down delay to meet youtube terms of use\n",
    "\"\"\"\n",
    "\n",
    "# set youtube channel name here\n",
    "channel_name = 'marquesbrownlee'\n",
    "\n",
    "\n",
    "youtube_base = 'https://www.youtube.com/'\n",
    "# others to try\n",
    "#  gotreehouse  howgrowvideo  gjenkinslbcc howgrowvideo\n",
    "# by channel name:\n",
    "#  UCu8YylsPiu9XfaQC74Hr_Gw  UCn34N9fj3x92kOGnQQdHkAQ\n",
    "# UCH4aPBlmmW1Vgs0ykktCMUg\n",
    "\n",
    "parent_folder = ''  # users or channel or empty\n",
    "\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"open url and return BeautifulSoup object, \n",
    "       or None if site does not exist\"\"\"\n",
    "    \n",
    "    result = requests.get(url)\n",
    "    if result.status_code != 200: return None\n",
    "    time.sleep(wait_between_requests)  # slow down to human speed\n",
    "    return BeautifulSoup(result.text, 'html.parser')\n",
    "\n",
    "\n",
    "def channel_section_links(channel_name):\n",
    "    \"\"\"list of \n",
    "       { 'title': <section title>, \n",
    "         'link': <url to section play lists> \n",
    "       }\"\"\"\n",
    "\n",
    "    global parent_folder\n",
    "\n",
    "    soup = get_soup(f'{youtube_base}/user/{channel_name}/playlists')\n",
    "    if soup is None or 'This channel does not exist.' in soup.text:\n",
    "        url = f'{youtube_base}/channel/{channel_name}/playlists'\n",
    "        soup = get_soup(url)\n",
    "        if soup is None or 'This channel does not exist.' in soup.text:\n",
    "            raise ValueError(\n",
    "                'The channel does not exists: ' + channel_name)\n",
    "        parent_folder = 'channel/'\n",
    "\n",
    "    play_list_atags = \\\n",
    "        soup.find_all('a',\n",
    "                      {'href': re.compile(f'{channel_name}/playlists')})\n",
    "    # filter out non user play lists next\n",
    "    elements = [{'title': x.text.strip(),\n",
    "                 'link': fix_url(x['href'])} for x in play_list_atags\n",
    "                if x.span and\n",
    "                ('shelf_id=0' not in x['href'])]\n",
    "\n",
    "    # no sections, make up no sections section with default link\n",
    "    if len(elements) == 0:\n",
    "        url = f'{youtube_base}{parent_folder}{channel_name}/playlists'\n",
    "        elements = [ {'title': 'no sections', 'link': url}]\n",
    "        # i.e.  https://youtube.com/gotreehouse/playlists\n",
    "    return elements\n",
    "\n",
    "\n",
    "def fix_url(url):  # correct relative urls back to absolute urls\n",
    "    if url[0] == '/':\n",
    "        return youtube_base + url\n",
    "    else:\n",
    "        return url\n",
    "\n",
    "\n",
    "def get_playlists(section):\n",
    "    \"\"\"returns list of list of\n",
    "    { 'title': <playlist tile>, <link to all playlist videos> }\"\"\"\n",
    "    global parent_folder\n",
    "    print(f\"  getting playlists for section: {section['title']}\")\n",
    "\n",
    "    soup = get_soup(section['link'])\n",
    "    if soup is None: # no playlist, create dummy with default link\n",
    "        url = f'{youtube_base}{parent_folder}{channel_name}/videos'\n",
    "        return [\n",
    "           {'title': 'No Playlists', 'link':url }]\n",
    "    atags = soup('a', class_='yt-uix-tile-link')\n",
    "\n",
    "    playlists = []\n",
    "    for a in atags:  # find title and link\n",
    "        title = a.text\n",
    "        if title != 'Liked videos': # skip these\n",
    "            url = fix_url(a['href'])\n",
    "            playlists.append({'title': title, 'link': url})\n",
    "\n",
    "    if not playlists:  # no playlists\n",
    "        url = f'{youtube_base}/{parent_folder}{channel_name}/videos'\n",
    "        return [{'title': 'No Playlists', 'link': url}]\n",
    "\n",
    "    return playlists\n",
    "\n",
    "def parse_video(vurl):\n",
    "    # return dict of\n",
    "    # title, link, views, publication_date,\n",
    "    # description, short_link, likes, dislikes\n",
    "\n",
    "    d = {'link': vurl, 'views': None, 'short_link': vurl,\n",
    "         'likes': None, 'dislikes': None}\n",
    "\n",
    "    # now get video page and pull information from it\n",
    "    vsoup = get_soup(vurl)\n",
    "\n",
    "    o = vsoup.find('title')\n",
    "    vtitle = o.text.strip()\n",
    "    xending = ' - YouTube'\n",
    "    d['title'] = vtitle[:-len(xending)] \\\n",
    "        if vtitle.endswith(xending) else vtitle\n",
    "    print(f\"      processing video '{d['title']}'\" )\n",
    "\n",
    "    # o is used in the code following to\n",
    "    # catch missing data targets for scrapping\n",
    "    o = vsoup.find('div', class_='watch-view-count')\n",
    "    if o:\n",
    "        views = o.text\n",
    "        d['views'] = ''.join(c for c in views if c in '0123456789')\n",
    "\n",
    "    o = vsoup.find('strong', class_='watch-time-text')\n",
    "    d['publication_date'] = \\\n",
    "        o.text[len('Published on ') - 1:] if o else ''\n",
    "\n",
    "    o = vsoup.find('div', id='watch-description-text')\n",
    "    d['description'] = o.text if o else ''\n",
    "\n",
    "    o = vsoup.find('meta', itemprop='videoId')\n",
    "    if o:\n",
    "        vid = o['content']\n",
    "        d['short_link'] = f'https://youtu.be/{vid}'\n",
    "\n",
    "    o = vsoup.find('button',\n",
    "                   class_='like-button-renderer-like-button')\n",
    "    if o:\n",
    "        o = o.find('span', class_='yt-uix-button-content')\n",
    "        d['likes'] = o.text if o else ''\n",
    "\n",
    "    o = vsoup.find('button',\n",
    "                   class_='like-button-renderer-dislike-button')\n",
    "    if o:\n",
    "        o = o.find('span', class_='yt-uix-button-content')\n",
    "        d['dislikes'] = o.text if o else ''\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def add_videos(playlist):\n",
    "    \"\"\"find videos in playlist[link]\n",
    "    and add their info as playlist[videos] as list\"\"\"\n",
    "    surl = playlist['link']\n",
    "    soup = get_soup(surl)\n",
    "    print(f\"    getting videos for playlist: {playlist['title']}\")\n",
    "\n",
    "    videos = []\n",
    "\n",
    "    # items are list of video a links from list\n",
    "    items = soup('a', class_='yt-uix-tile-link')\n",
    "\n",
    "    # note first part of look get info from playlist page item,\n",
    "    # and the the last part opens the video and gets more details\n",
    "    if len(items) > 0:\n",
    "        for i in items:\n",
    "            d = dict()\n",
    "            vurl = fix_url(i['href'])\n",
    "            t = i.find_next('span', {'aria-label': True})\n",
    "            d['time'] = t.text if t else 'NA'\n",
    "\n",
    "            d.update(parse_video(vurl))\n",
    "            videos.append(d)\n",
    "\n",
    "    else:  # must be only one video\n",
    "        d = {'time': 'NA'}\n",
    "        d.update(parse_video(surl))\n",
    "        videos.append(d)\n",
    "\n",
    "    # add new key to this playlist of list of video infos\n",
    "    playlist['videos'] = videos\n",
    "    print()\n",
    "\n",
    "\n",
    "def tag(t,c):\n",
    "    return f'<{t}>{c}</{t}>' # return html tag with content\n",
    "\n",
    "\n",
    "def link(text, url): # return a tag with content and link\n",
    "    return f'<a href=\"{url}\">{text}</a>'\n",
    "\n",
    "\n",
    "def html_out(channel, sections):\n",
    "    \"\"\"create and write channel_name.html file\"\"\"\n",
    "    title = f'YouTube Channel {channel}'\n",
    "     \n",
    "\n",
    "    f = open(f'{channel}.html','w',encoding=\"utf-8\")\n",
    "    template = ('<!doctype html>\\n<html lang=\"en\">\\n<head>\\n'\n",
    "                '<meta charset=\"utf-8\">'\n",
    "                '<title>{}</title>\\n</head>\\n'\n",
    "                '<body>\\n{}\\n</body>\\n</html>')\n",
    "\n",
    "    parts = list()\n",
    "    parts.append(tag('h1', title))\n",
    "\n",
    "    for s in sections:\n",
    "        parts.append(tag('h2',link(s['title'], s['link'])))\n",
    "        for pl in s['playlists']:\n",
    "            parts.append(tag('h3', link(pl['title'], pl['link'])))\n",
    "            if len(pl) == 0:\n",
    "                parts.append('<p>Empty Playlist</p>')\n",
    "            else:\n",
    "                parts.append('<ol>')\n",
    "                for v in pl['videos']:\n",
    "                    t = '' if v['time'] == 'NA' else f\" ({v['time']})\"\n",
    "                    parts.append(tag('li', link(v['title'],\n",
    "                                     v['short_link']) + t))\n",
    "                parts.append('</ol>')\n",
    "    f.write(template.format(channel, '\\n'.join(parts)))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def csv_out(channel, sections):\n",
    "    \"\"\" create and output channel_name.csv\n",
    "    file for import into a spreadsheet or DB\"\"\"\n",
    "    headers = ('channel,section,playlist,video,'\n",
    "               'link,time,views,publication date,'\n",
    "               'likes,dislikes,description').split(',')\n",
    "\n",
    "    with open(f'{channel}.csv', 'w', encoding=\"utf-8\") as csv_file:\n",
    "        csvf = csv.writer(csv_file, delimiter=',')\n",
    "        csvf.writerow(headers)\n",
    "        for section in sections:\n",
    "            for playlist in section['playlists']:\n",
    "                for video in playlist['videos']:\n",
    "                    v = video\n",
    "                    line = [channel,\n",
    "                            section['title'],\n",
    "                            playlist['title'],\n",
    "                            v['title']]\n",
    "                    line.extend([v['short_link'],\n",
    "                                 v['time'], v['views'],\n",
    "                                 v['publication_date'],\n",
    "                                 v['likes'], v['dislikes'],\n",
    "                                 v['description']])\n",
    "                    csvf.writerow(line)\n",
    "\n",
    "def process_channel(channel_name):\n",
    "    sections = channel_section_links(channel_name)\n",
    "    for section in sections:\n",
    "        section['playlists'] = get_playlists(section)\n",
    "        for playlist in section['playlists']:\n",
    "            add_videos(playlist)\n",
    "    return sections\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # find channel name by going to channel\n",
    "    # and picking last element from channel url\n",
    "    # for example my channel url is:\n",
    "    #   https://www.youtube.com/user/gjenkinslbcc\n",
    "    # my channel name is gjenkinslbcc in this url\n",
    "    # this is set near top of this file\n",
    "    # if the channel is of the form:\n",
    "    # https://www.youtube.com/channel/xyz then supply xyz\n",
    "\n",
    "    print(f'finding sections for youtube.com {channel_name}')\n",
    "    sections = process_channel(channel_name)\n",
    "\n",
    "    # save sections structure to json file\n",
    "    with open(f'{channel_name}.json','w') as f:\n",
    "        f.write(json.dumps(sections, sort_keys=True, indent=4))\n",
    "\n",
    "    html_out(channel_name, sections)  # create web page of channel links\n",
    "\n",
    "    # create a csv file of video info for import into spreadsheet\n",
    "    csv_out(channel_name, sections)\n",
    "\n",
    "    print(f\"Program Complete,\\n  '{channel_name}.html' and\"\n",
    "          f\" '{channel_name}.csv' have been\" \n",
    "          f\" written to current directory\")\n",
    "    # 3456789012345678901234567890123456789012345678901234567890123456789012345678901234567890\n",
    "    #        1         2         3         4         5         6         7         8         9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
