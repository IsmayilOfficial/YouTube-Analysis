---
title: "AnalysisPart2"
author: "Ismayil Tahmazov"
date: "October 18, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r library}

library(httr)
library(tuber)
library(lubridate)
library(gridExtra)
library(tidyverse)
library(stringi)
library(wordcloud)
library(gridExtra)

```


```{r}
app_id = '513902774051-nfunqdrtndpb7jkp1t069rbcut20vif6.apps.googleusercontent.com'
app_secret = '1Tw5Sv95wRU1K5zV3FiwMfLG'




# establishing connecting with YouTube




# credentials 

yt_oauth(app_id = app_id, app_secret = app_secret)

```










```{r}

 

 videos <- read.csv("music2019.csv")
xax<- head(videos,1)
  
comments = lapply(as.character(videos$video_id), function(x){
  get_comment_threads(c(video_id = "VIWVfkF2IeI"), max_results = 100)
})

comments = lapply(as.character(xax$video_id), function(x){
  get_comment_threads(c(video_id = x), max_results = 20)
})
 


```
```{r}
install.packages("SocialMediaLab")
library(SocialMediaLab)
```


```{r}
comments<- read.csv("desertRoseComments.csv")
## = WordClouds = ##

comments_text = tibble(text = Reduce(c, comments$Text)) %>%
  mutate(text = stri_trans_general(tolower(text), "Latin-ASCII"))
remove = c("you","the","que","and","your","muito","this","that","are","for","cara",
         "from","very","like","have","voce","man","one","nao","com","with","mais",
         "was","can","uma","but","ficou","meu","really","seu","would","sua","more",
         "it's","it","is","all","i'm","mas","como","just","make","what","esse","how",
         "por","favor","sempre","time","esta","every","para","i've","tem","will",
         "you're","essa","not","faz","pelo","than","about","acho","isso",
         "way","also","aqui","been","out","say","should","when","did","mesmo",
         "minha","next","cha","pra","sei","sure","too","das","fazer","made",
         "quando","ver","cada","here","need","ter","don't","este","has","tambem",
         "una","want","ate","can't","could","dia","fiquei","num","seus","tinha","vez",
         "ainda","any","dos","even","get","must","other","sem","vai","agora","desde","haytraochoanh",
         "dessa","fez","many","most","tao","then","tudo","vou","ficaria","foi","pela","hay","100m","qua","<U+C544><U+B2CC><U+C904>","dong","100tr","khong","thanh","thuat","trang","quyen","trong","chinh","g2019","mexico","tikixcurnondicungmin","<U+C544><U+B2CC><U+C904>","vong","tieng","miniacs","bit.ly","ten","mong","coa","aaa","quen","min","chi","hat","lam","ban","duac","nai","nghe","lai","roi","nai","bai","ngan","nai","xem","view","nay","yeu","cung","day","100",
         "see","teu","those","were")
words = tibble(word = Reduce(c, stri_extract_all_words(comments$Text))) %>%
  group_by(word) %>% count() %>% arrange(desc(n)) %>% filter(nchar(word) >= 2) %>%
  filter(n > 10 & word %in% remove == FALSE) 
 
set.seed(3)
wordcloud(words$word, words$n, random.order = FALSE, random.color = TRUE,
          rot.per = 0.3, colors = 1:nrow(words))
```